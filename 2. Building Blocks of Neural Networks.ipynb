{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Neural Network with Numpy and Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept of Perceptron\n",
    "Data, like test scores and grades, is fed into a network of interconnected nodes. These individual nodes are called perceptrons or neurons, and they are the basic unit of a neural network. Each one looks at input data and decides how to categorize that data. In the example above, the input either passes a threshold for grades and test scores or doesn't, and so the two categories are: yes (passed the threshold) and no (didn't pass the threshold). These categories then combine to form a decision -- for example, if both nodes produce a \"yes\" output, then this student gains admission into the university.\n",
    "\n",
    "When we initialize a neural network, we don't know what information will be most important in making a decision. It's up to the neural network to learn for itself which data is most important and adjust how it considers that data.\n",
    "\n",
    "It does this with something called __weights__.\n",
    "\n",
    "### Weights\n",
    "When input data comes into a perceptron, it gets multiplied by a weight value that is assigned to this particular input. For example, the perceptron for the above problem would have two inputs, _tests_ for test scores and _grades_, so it has two associated weights that can be adjusted individually. These weights start out as random values, and as the neural network network learns more about what kind of input data leads to a student being accepted into a university, the network adjusts the weights based on any errors in categorization that the previous weights resulted in. This is called __training__ the neural network.\n",
    "\n",
    "A higher weight means the neural network considers that input more important than other inputs, and lower weight means that the data is considered less important. An extreme example would be if test scores had no affect at all on university acceptance; then the weight of the test score input data would be zero and it would have no affect on the output of the perceptron.\n",
    "\n",
    "### Summing up the Input data\n",
    "\n",
    "So, each input to a perceptron has an associated weight that represents its importance and these weights are determined during the learning process of a neural network, called training. In the next step, the weighted input data is summed up to produce a single value, that will help determine the final output - whether a student is accepted to a university or not. \n",
    "\n",
    "When writing equations related to neural networks, the weights will always be represented by some type of the letter _w_. It will usually look like a _W_ when it represents a matrix of weights or a _w_ when it represents an individual weight, and it may include some additional information in the form of a subscript to specify which weights. But remember, when you see the letter _w_, think weights.\n",
    "\n",
    "\n",
    "\n",
    "In this example, we'll use $w_{grades}$ for the _grades_ and $w_{test}$ for the weight of test. Let's say that the weights are: $w_{grades} = -1, w_{test} = -0.2$. You don't have to be concerned with the actual values, but their relative values are important. $w_{grades}$ is 5 times larger than $w_{test}$, which means the neural network considers _grades_ input 5 times more important than _test_ in determining whether a student will be accepted into a university.\n",
    "\n",
    "The perceptron applies these weights to the inputs and sums them in a process known as __linear combination__. \n",
    "In our case, this looks like $w_{grades}⋅x_{grades} + w_{test}⋅x_{test} = -1⋅x_{grades} + -0.2⋅x_{test}$\n",
    "\n",
    "In this example, we just have 2 simple inputs: _grades_ and _tests_. Let's imagine we instead had _m_ different inputs and we labeled them $x_{1},x_{2},\\dots,x_{m}$. Let's also say that the weight corresponding to $x_{1}$ is $w_{1}$ and so on. In that case, we would express the linear combination succintly as $\\sum_{i=1}^{m} w_i⋅x_i$. Here, the Greek letter $\\Sigma$ is used to represent summation. It simply means to evaluate the equation to the right multiple times and add up the results. In this case, the equation it will sum is $w_i⋅x_i$\n",
    "\n",
    "But where do we get $w_i$ and $x_i$? $\\sum_{i=1}^{m}$  means to iterate over all i values, from 1 to m.\n",
    "So to put it all together, $\\sum_{i=1}^{m} w_i⋅x_i$ means the following:\n",
    "\n",
    "- Start at _i = 1_\n",
    "- Evaluate $w_1⋅x_1$ and remember the results\n",
    "- Move to _i = 2_\n",
    "- Evaluate $w_2⋅x_2$ and add these results to $w_1⋅x_1$ \n",
    "- Continue repeating that process until _i = m_, where _m_ is the number of inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Output with an Activation Function\n",
    "Finally, the result of the perceptron's summation is turned into an output signal! This is done by feeding the linear combination into an __activation function__.\n",
    "\n",
    "Activation functions are functions that decide, given the inputs into the node, what should be the node's output? Because it's the activation function that decides the actual output, we often refer to the outputs of a layer as its \"activations\".\n",
    "\n",
    "One of the simplest activation functions is the _Unit step function_. This function returns a 0 if the linear combination is less than 0. It returns a 1 if the linear combination is positive or equal to zero. \n",
    "\n",
    "$$\n",
    "f(x) = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            0 & \\quad x < 0 \\\\\n",
    "            1 & \\quad x \\geq 0\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "$$\n",
    "\n",
    "In the university acceptance example above, we used the weights $w_{grades} = -1, w_{test} = -0.2$. Since $w_{grades}$ and $w_{test}$ are negative values, the activation function will only return a 1 if _grades_ and _test_ are 0! This is because the range of values from the linear combination using these weights and inputs are (−∞,0].\n",
    "\n",
    "Now remember that the step activation function returns 1 for any inputs greater than or equal to zero. Hence, only the output with value 0 will be accepted to the university. Now, we certainly want more than one possible grade/test combination to result in acceptance, so we need to adjust the results passed to our activation function so it activates – that is, returns 1 – for more inputs. Specifically, we need to find a way so all the scores we’d like to consider acceptable for admissions produce values greater than or equal to zero when linearly combined with the weights into our node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias\n",
    "One way to get our function to return 1 for more inputs is to add a value to the results of our linear combination, called a __bias__.\n",
    "\n",
    "A bias, represented in equations as $b$, lets us move values in one direction or another.\n",
    "\n",
    "Of course, with neural networks we won't know in advance what values to pick for biases. That’s ok, because just like the weights, the bias can also be updated and changed by the neural network during training. So after adding a bias, we now have a complete perceptron formula:\n",
    "\n",
    "$$\n",
    "f(x_1,x_2,\\dots,x_m) = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            0 \\quad if \\ b + \\sum_{i=1}^{m} w_i⋅x_i   < 0 \\\\\n",
    "            1 \\quad if \\ b + \\sum_{i=1}^{m} w_i⋅x_i  \\geq 0\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "$$\n",
    "\n",
    "\n",
    "This formula returns 1 if the input $x_1,x_2,\\dots,x_m$ belongs to the accepted-to-university category or returns 0 if it doesn't. The input is made up of one or more real numbers, each one represented by $x_i$, where $m$ is the number of inputs.\n",
    "\n",
    "Then the neural network starts to learn! Initially, the weights $w_i$ and bias $b$ are assigned a random value, and then they are updated using a learning algorithm like _gradient descent_. The weights and biases change so that the next training example is more accurately categorized, and patterns in data are \"learned\" by the neural network.\n",
    "\n",
    "In the next section, you'll create the AND perceptron by setting the values for weights and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input 1</th>\n",
       "      <th>Input 2</th>\n",
       "      <th>Linear Combination</th>\n",
       "      <th>Activation Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Input 1  Input 2  Linear Combination  Activation Output\n",
       "0        0        0                  -2                  0\n",
       "1        0        1                  -1                  0\n",
       "2        1        0                  -1                  0\n",
       "3        1        1                   0                  1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#AND Perceptron\n",
    "\n",
    "def activation_function(input):\n",
    "    return 1 if int(input)>=0 else 0\n",
    "weight1 = 1\n",
    "weight2 = 1\n",
    "bias = -2\n",
    "\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "outputs = []\n",
    "\n",
    "# Generate and check output\n",
    "for test_input in test_inputs:\n",
    "    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias #w1x1 + w2x2 + b\n",
    "    output = activation_function(linear_combination) # activation function\n",
    "    outputs.append([test_input[0], test_input[1], linear_combination, output])\n",
    "\n",
    "output_frame = pd.DataFrame(outputs, columns=['Input 1', 'Input 2', 'Linear Combination', 'Activation Output'])\n",
    "output_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are two ways to go from an AND perceptron to an OR perceptron?\n",
    "- Increase the weights\n",
    "- Decrease the bias\n",
    "\n",
    "Try to create an OR perceptron on your own"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Perceptron with Numpy\n",
    "\n",
    "So far you've been working with perceptrons where the output is always one or zero. The input to the output unit is passed through an activation function, $f(h)$, in this case, the step function.\n",
    "The output unit returns the result of $f(h)$, where $h$ is the input to the output unit: $h = \\sum_{i=1}^{m} w_i⋅x_i + b$.\n",
    "Hence output $y = f(\\sum_{i=1}^{m} w_i⋅x_i + b)$\n",
    "\n",
    "The cool part about this architecture, and what makes neural networks possible, is that the activation function, $f(h)$ can be any function, not just the step function shown earlier.\n",
    "\n",
    "For example, if you let $f(h) = h$, the output will be the same as the input. Now the output of the network is $y = \\sum_{i=1}^{m} w_i⋅x_i + b$\n",
    "\n",
    "This equation should be familiar to you, it's the same as the linear regression model!\n",
    "\n",
    "Other activation functions you'll see are the logistic (often called the sigmoid), tanh, and softmax functions. We'll mostly be using the sigmoid function for the rest of this lesson:\n",
    "\n",
    "#### Sigmoid function\n",
    "$sigmoid(x) = \\frac {1}{1+e^{-x}}$\n",
    "\n",
    "The sigmoid function is bounded between 0 and 1, and as an output can be interpreted as a probability for success. It turns out, again, using a sigmoid as the activation function results in the same formulation as logistic regression.\n",
    "\n",
    "This is where it stops being a perceptron and begins being called a neural network. In the case of simple networks like this, neural networks don't offer any advantage over general linear models such as logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43290709503454572"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sigmoid is the activation function\n",
    "def sigmoid(x): \n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "inputs = np.array([0.7, -0.3])\n",
    "weights = np.array([0.1, 0.8])\n",
    "bias = -0.1\n",
    "\n",
    "output = sigmoid(np.dot(weights, inputs) + bias)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning weights\n",
    "You've seen how you can use perceptrons for AND and XOR operations, but there we set the weights by hand. What if you want to perform an operation, such as predicting college admission, but don't know the correct weights? You'll need to learn the weights from example data, then use those weights to make the predictions.\n",
    "\n",
    "To figure out how we're going to find these weights, start by thinking about the goal. We want the network to make predictions as close as possible to the real values. To measure this, we need a metric of how wrong the predictions are, the __error__. \n",
    "\n",
    "A common metric is the sum of the squared errors (SSE):\n",
    "$$E = \\frac{1}{2}\\sum_\\mu \\sum_j \\left[ y^{\\mu}_j - \\hat{y} ^{\\mu} _j \\right]^2$$\n",
    "\n",
    "where $\\hat y$ is the prediction and $y$ is the true value, and you take the sum over all output units $j$ and another sum over all data points $\\mu$. This might seem like a really complicated equation at first, but it's fairly simple once you understand the symbols and can say what's going on in words.\n",
    "\n",
    "First, the inside sum over $j$. This variable $j$ represents the output units of the network. So this inside sum is saying for each output unit, find the difference between the true value $y$ and the predicted value from the network $\\hat y$, then square the difference, then sum up all those squares.\n",
    "\n",
    "Then the other sum over $\\mu$ is a sum over all the data points. So, for each data point you calculate the inner sum of the squared differences for each output unit. Then you sum up those squared differences for each data point. That gives you the overall error for all the output predictions for all the data points.\n",
    "The SSE is a good choice for a few reasons. The square ensures the error is always positive and larger errors are penalized more than smaller errors. Also, it makes the math nice, always a plus.\n",
    "\n",
    "Remember that the output of a neural network, the prediction, depends on the weights\n",
    "$$\\hat{y}^{\\mu}_j = f \\left( \\sum_i{ w_{ij} x^{\\mu}_i  }\\right)$$\n",
    "and accordingly the error depends on the weights\n",
    "$$E = \\frac{1}{2}\\sum_\\mu \\sum_j \\left[ y^{\\mu}_j - f \\left( \\sum_i{ w_{ij} x^{\\mu}_i  }\\right) \\right]^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "We want the network's prediction error to be as small as possible and the weights are the knobs we can use to make that happen. Our goal is to find weights $w_{ij}$  that minimize the squared error $E$. To do this with a neural network, typically you'd use __gradient descent__. With gradient descent, we take multiple small steps towards our goal. In this case, we want to change the weights in steps that reduce the error. \n",
    "\n",
    "Gradient is another term for rate of change or slope. \n",
    "$$E = \\frac{1}{2}\\sum_\\mu{(y^\\mu - \\hat y^\\mu)^2}$$\n",
    "\n",
    "$$E = \\frac{1}{2}\\sum_\\mu \\sum_j \\left[ y^{\\mu}_j - f \\left( \\sum_i{ w_{ij} x^{\\mu}_i  }\\right) \\right]^2$$\n",
    "\n",
    "$$w_i = w_i + \\Delta w_i$$\n",
    "\n",
    "$$w_i - \\frac {\\partial E}{\\partial w_i}$$\n",
    "\n",
    "$$w_i = - \\eta⋅ \\frac{\\partial E}{\\partial w_i}$$\n",
    "\n",
    "Here $\\eta$ is called the __learning rate__.\n",
    "\n",
    "$$\\frac {\\partial E}{\\partial w_i} = \\frac {\\partial}{\\partial w_i}\\frac {1}{2}(y - \\hat y)^2$$\n",
    "\n",
    "\n",
    "$$By \\ Chain \\ Rule, \\ \\frac {\\partial}{\\partial z}p(q(z)) = \\frac {\\partial p}{\\partial q}\\frac {\\partial q}{\\partial z}$$\n",
    "\n",
    "$$\\frac {\\partial E}{\\partial w_i} = (y - \\hat y) \\frac {\\partial}{\\partial w_i}(y - \\hat y) \\quad (Taking \\ q = (y - \\hat y) \\ and \\ p = \\frac {1}{2} q (w_i)^2$$ \n",
    "\n",
    "\n",
    "$$\\frac {\\partial E}{\\partial w_i} = - (y - \\hat y) \\frac {\\partial}{\\partial w_i}(y - \\hat y) = (y - \\hat y) \\frac {\\partial \\hat y}{\\partial w_i}$$\n",
    "\n",
    "\n",
    "$$\\hat y = f(h) \\ where \\ h = \\sum_i{ w_i x_i}, \\ so \\ \\frac {\\partial E}{\\partial w_i} = (y - \\hat y) \\frac {\\partial \\hat y}{\\partial w_i} \\ , hence \\ \\frac {\\partial E}{\\partial w_i} = (y - \\hat y)⋅f'(h)⋅\\frac {\\partial}{\\partial w_i}\\sum_i{ w_i x_i}$$\n",
    "\n",
    "$$Now \\ \\frac {\\partial}{\\partial w_i}(\\sum_i{ w_i x_i}) = \\frac {\\partial}{\\partial w_1}(w_1 x_1 + w_2 x_2 + \\dots + w_m x_m) = x_1 + 0 + 0 + \\dots + 0$$\n",
    "\n",
    "$$Hence \\ \\frac {\\partial}{\\partial w_i}(\\sum_i{ w_i x_i}) = x_i$$\n",
    "\n",
    "$$\\frac {\\partial E}{\\partial w_i} = - (y - \\hat y)⋅f'(h)⋅x_i$$\n",
    "\n",
    "$$\\Delta w_i = \\eta⋅(y - \\hat y)⋅f'(h)⋅x_i$$\n",
    "\n",
    "$$\\delta = (y - \\hat y)⋅f'(h)$$\n",
    "\n",
    "$$w_i = w_i + \\Delta w_i = w_i + \\eta⋅(y - \\hat y)⋅f'(h)⋅x_i = w_i + \\eta⋅\\delta⋅x_i$$\n",
    "\n",
    "### Implementing Gradient Descent for single output from pair of inputs\n",
    "\n",
    "One weight update can be calculated as: $\\Delta w_i = \\eta \\, \\delta x_i$ and error term $\\delta$ can be calculated as:\n",
    "$$\\delta = (y - \\hat y)⋅f'(h) =  (y - \\hat y)⋅f'(\\sum w_i⋅x_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network output:\n",
      "0.377540668798\n",
      "Amount of Error:\n",
      "0.122459331202\n",
      "Change in Weights:\n",
      "[ 0.01477465  0.0295493 ]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))\n",
    "\n",
    "learnrate = 0.5\n",
    "x,y = np.array([1, 2]), np.array(0.5)\n",
    "\n",
    "# Initial weights\n",
    "w = np.array([0.5, -0.5])\n",
    "\n",
    "# Calculate one gradient descent step for each weight\n",
    "nn_output = sigmoid(np.dot(x, w))\n",
    "\n",
    "# Calculate error of neural network\n",
    "error = y - nn_output\n",
    "\n",
    "# Calculate the error term\n",
    "error_term = error * sigmoid_derivative(nn_output)\n",
    "\n",
    "# Calculate change in weights\n",
    "del_w = learnrate * error_term * x\n",
    "\n",
    "print('Neural Network output:')\n",
    "print(nn_output)\n",
    "print('Amount of Error:')\n",
    "print(error)\n",
    "print('Change in Weights:')\n",
    "print(del_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a neural network with a single layer \n",
    "As an example, you'll use gradient descent to train a network on graduate school admissions data (found at http://www.ats.ucla.edu/stat/data/binary.csv. This dataset has three input features: GRE score, GPA, and the rank of the undergraduate school (numbered 1 through 4). Institutions with rank 1 have the highest prestige, those with rank 4 have the lowest.\n",
    "\n",
    "You might think there will be three input units, but we actually need to transform the data first. The rank feature is categorical, the numbers don't encode any sort of relative values. Rank 2 is not twice as much as rank 1, rank 3 is not 1.5 more than rank 2. Instead, we need to use dummy variables to encode rank, splitting the data into four new columns encoded with ones or zeros. Rows with rank 1 have one in the rank 1 dummy column, and zeros in all other columns. Rows with rank 2 have one in the rank 2 dummy column, and zeros in all other columns. And so on.\n",
    "\n",
    "We'll also need to standardize the GRE and GPA data, which means to scale the values such they have zero mean and a standard deviation of 1. This is necessary because the sigmoid function squashes really small and really large inputs. The gradient of really small and large inputs is zero, which means that the gradient descent step will go to zero too. Since the GRE and GPA values are fairly large, we have to be really careful about how we initialize the weights or the gradient descent steps will die off and the network won't train. Instead, if we standardize the data, we can initialize the weights easily and everyone is happy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>rank_1</th>\n",
       "      <th>rank_2</th>\n",
       "      <th>rank_3</th>\n",
       "      <th>rank_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>-0.066657</td>\n",
       "      <td>0.289305</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>0.625884</td>\n",
       "      <td>1.445476</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.837832</td>\n",
       "      <td>1.603135</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>1.318426</td>\n",
       "      <td>-0.131120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>-0.066657</td>\n",
       "      <td>-1.208461</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          gre       gpa  rank_1  rank_2  rank_3  rank_4\n",
       "209 -0.066657  0.289305       0       1       0       0\n",
       "280  0.625884  1.445476       0       1       0       0\n",
       "33   1.837832  1.603135       0       0       1       0\n",
       "210  1.318426 -0.131120       0       0       0       1\n",
       "93  -0.066657 -1.208461       0       1       0       0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV data into a Pandas Dataframe\n",
    "admissions = pd.read_csv('Data/binary.csv')\n",
    "\n",
    "# Make dummy variables for rank\n",
    "data = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
    "data = data.drop('rank', axis=1)\n",
    "\n",
    "# Standarize features\n",
    "for field in ['gre', 'gpa']:\n",
    "    mean, std = data[field].mean(), data[field].std()\n",
    "    data.loc[:,field] = (data[field]-mean)/std\n",
    "    \n",
    "# Split off random 10% of the data for testing\n",
    "np.random.seed(42)\n",
    "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
    "data, test_data = data.ix[sample], data.drop(sample)\n",
    "\n",
    "# Split into features and targets\n",
    "features, targets = data.drop('admit', axis=1), data['admit']\n",
    "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the general algorithm for updating the weights with gradient descent:\n",
    "\n",
    "- Set the weight step to zero: $Δw_i = 0$\n",
    "- For each record in the training data:\n",
    "    - Make a forward pass through the network, calculating the output $\\hat y = f(\\sum_i w_i x_i)$\n",
    "    - Calculate the error gradient in the output unit, $\\delta = (y - \\hat y) * f'(\\sum_i w_i x_i)$\n",
    "    - Update the weight step $\\Delta w_i =  \\Delta w_i + \\delta x_i$\n",
    "- Update the weights $w_i = w_i + \\eta \\Delta w_i / m$ where $\\eta$ is the learning rate and $m$ is the number of records. Here we're averaging the weight steps to help reduce any large variations in the training data.\n",
    "- Repeat for $e$ epochs.\n",
    "\n",
    "You can also update the weights on each record instead of averaging the weight steps after going through all the records.\n",
    "\n",
    "Remember that we're using the sigmoid for the activation function, $f(h) = 1/(1+e^{-h})$\n",
    "And the gradient of the sigmoid is $f'(h) = f(h) (1 - f(h))$\n",
    "where $h$ is the input to the output unit,\n",
    "$h = \\sum_i w_i x_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train loss: 0.263905014894 Change: 0.263905014894\n",
      "Epoch 100 - Train loss: 0.242328506085 Change: -0.0215765088097\n",
      "Epoch 200 - Train loss: 0.228977632706 Change: -0.0133508733785\n",
      "Epoch 300 - Train loss: 0.220064954771 Change: -0.00891267793458\n",
      "Epoch 400 - Train loss: 0.213905490575 Change: -0.00615946419639\n",
      "Epoch 500 - Train loss: 0.209572672658 Change: -0.00433281791669\n",
      "Epoch 600 - Train loss: 0.206483823096 Change: -0.00308884956207\n",
      "Epoch 700 - Train loss: 0.204252876878 Change: -0.00223094621882\n",
      "Epoch 800 - Train loss: 0.202619677422 Change: -0.00163319945557\n",
      "Epoch 900 - Train loss: 0.201407559916 Change: -0.00121211750632\n",
      "Prediction accuracy: 0.750\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))\n",
    "\n",
    "# Use to same seed to make debugging easier\n",
    "np.random.seed(42)\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = 0.0\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.random.normal(scale=1/n_features**.5, size=n_features)\n",
    "\n",
    "# Neural Network parameters\n",
    "epochs = 1000\n",
    "learnrate = 0.1\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w = np.zeros(weights.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        # Loop through all records, x is the input, y is the target\n",
    "\n",
    "        # Activation of the output unit\n",
    "        output = sigmoid(np.dot(x, weights))\n",
    "\n",
    "        # The error, the target minues the network output\n",
    "        error = y - output\n",
    "\n",
    "        # The gradient descent step, the error times the gradient times the inputs\n",
    "        del_w += error * sigmoid_derivative(output) * x\n",
    "\n",
    "        # Update the weights here. The learning rate times the \n",
    "        # change in weights, divided by the number of records to average\n",
    "    weights += learnrate * del_w / n_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        out = sigmoid(np.dot(features, weights))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Epoch {} - Train loss: {} Change: {} - WARNING - Loss Increasing\".format(e,loss, loss - last_loss))\n",
    "        else:\n",
    "            print(\"Epoch {} - Train loss: {} Change: {}\".format(e,loss, loss - last_loss))\n",
    "        last_loss = loss\n",
    "\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "tes_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network with a hidden layer\n",
    "Before, we were dealing with only one output node which made the code straightforward. However now that we have multiple input units and multiple hidden units, the weights between them will require two indices: $w_{ij}$ where $i$ denotes input units and $j$ are the hidden units.\n",
    "Let us have an input layer $x_1, x_2, x_3$ and a hidden layer $h_1, h_2$ with the output of hidden layer giving us the output $y$.\n",
    "\n",
    "Now to index the weights, we take the input unit number for the $i$ and the hidden unit number for the $j$. That gives us $w_1$ for the weight leading from $x_1$ to $h_1$ and $w_{12}$ for the weight leading from $x_1$ to $h_2$ and so on.\n",
    "Before, we were able to write the weights as an array, indexed as $w_i$.\n",
    "\n",
    "But now, the weights need to be stored in a matrix, indexed as $w_{ij}$. Each row in the matrix will correspond to the weights leading out of a single input unit, and each column will correspond to the weights leading in to a single hidden unit. For our three input units and two hidden units, the weights matrix looks like this:\n",
    "\n",
    "\n",
    "Remember how the input to a hidden unit is the sum of all the inputs multiplied by the hidden unit's weights. So for each hidden layer unit, $$h_j = \\sum_i{ w_{ij} x_i  }$$\n",
    "\n",
    "To do that, we now need to use matrix multiplication. \n",
    "In this case, we're multiplying the inputs (a row vector here) by the weights. To do this, you take the dot (inner) product of the inputs with each column in the weights matrix. For example, to calculate the input to the first hidden unit, j=1, you'd take the dot product of the inputs with the first column of the weights matrix. \n",
    "\n",
    "#### Forward pass\n",
    "And for the second hidden layer input, you calculate the dot product of the inputs with the second column. And so on and so forth.\n",
    "\n",
    "In Numpy, you can do this for all the inputs and all the outputs at once using np.dot()\n",
    "\n",
    "$$hidden\\_inputs = np.dot(inputs, weights\\_input\\_to\\_hidden)$$\n",
    "You could also define your weights matrix such that it has dimensions n_hidden by n_inputs then multiply like so where the inputs form a column vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hidden-layer Output:', array([ 0.41492192,  0.42604313,  0.5002434 ]))\n",
      "('Output-layer Output:', array([ 0.49815196,  0.48539772]))\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Network size\n",
    "N_input = 4\n",
    "N_hidden = 3\n",
    "N_output = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make some fake data\n",
    "X = np.random.randn(4)\n",
    "\n",
    "weights_in_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))\n",
    "weights_hidden_out = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))\n",
    "\n",
    "# Make a forward pass through the network\n",
    "\n",
    "hidden_layer_in = np.dot(X, weights_in_hidden)\n",
    "hidden_layer_out = sigmoid(hidden_layer_in)\n",
    "\n",
    "print('Hidden-layer Output:', hidden_layer_out)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_out, weights_hidden_out)\n",
    "output_layer_out = sigmoid(output_layer_in)\n",
    "\n",
    "print('Output-layer Output:', output_layer_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation through a neural network\n",
    "Now we've come to the problem of how to make a multilayer neural network learn. Before, we saw how to update weights with gradient descent. The backpropagation algorithm is just an extension of that, using the chain rule to find the error with the respect to the weights connecting the input layer to the hidden layer (for a two layer network).\n",
    "\n",
    "To update the weights to hidden layers using gradient descent, you need to know how much error each of the hidden units contributed to the final output. Since the output of a layer is determined by the weights between layers, the error resulting from units is scaled by the weights going forward through the network. Since we know the error at the output, we can use the weights to work backwards to hidden layers.\n",
    "\n",
    "For example, in the output layer, you have errors $\\delta^o_k$ attributed to each output unit $k$. Then, the error attributed to hidden unit $j$ is the output errors, scaled by the weights between the output and hidden layers (and the gradient):\n",
    "$$\n",
    "\\delta^h_j = \\sum{W_{jk} \\delta^o_k f'(h_j)}\n",
    "$$\n",
    "\n",
    "Then, the gradient descent step is the same as before, just with the new errors: \n",
    "$$\n",
    "\\Delta w_{ij} = \\eta \\delta^h_j x_i\n",
    "$$\n",
    "where $w_{ij}$ are the weights between the inputs and hidden layer and $x_i$ are input unit values. This form holds for however many layers there are. The weight steps are equal to the step size times the output error of the layer times the values of the inputs to that layer:\n",
    "$$\n",
    "\\Delta W_{pq} = \\eta \\delta_{output} V_{in}\n",
    "$$\n",
    "Here, you get the output error, $\\delta_{output}$, by propagating the errors backwards from higher layers. And the input values, $V_{in}$ are the inputs to the layer, the hidden layer activations to the output unit for example.\n",
    "\n",
    "#### Illustrative Example\n",
    "\n",
    "Let's walk through the steps of calculating the weight updates for a simple two layer network. Suppose there are two input values, one hidden unit, and one output unit, with sigmoid activations on the hidden and output units.  \n",
    "\n",
    "Assume we're trying to fit some binary data and the target is y=1. We'll start with the forward pass, first calculating the input to the hidden unit\n",
    "$$h = \\sum_i w_i x_i = 0.1 \\times 0.4 - 0.2 \\times 0.3 = -0.02$$\n",
    "\n",
    "and the output of the hidden unit\n",
    "$$a = f(h) = \\mathrm{sigmoid}(-0.02) = 0.495$$\n",
    "\n",
    "Using this as the input to the output unit, the output of the network is\n",
    "\n",
    "$$\\hat y = f(W \\cdot a) = \\mathrm{sigmoid}(0.1 \\times 0.495) = 0.512$$\n",
    "\n",
    "With the network output, we can start the backwards pass to calculate the weight updates for both layers. Using the fact that for the sigmoid function \n",
    "$$f'(W \\cdot a) = f(W \\cdot a) (1 - f(W \\cdot a))$$\n",
    "\n",
    "The error for the output unit is\n",
    "$$ \\delta^o = (y - \\hat y) f'(W \\cdot a) = (1 - 0.512) \\times 0.512 \\times(1 - 0.512) = 0.122$$\n",
    "\n",
    "Now we need to calculate the error for the hidden unit with backpropagation. Here we'll scale the error from the output unit by the weight W connecting it to the hidden unit. For the hidden unit error, $\\delta^h_j = \\sum_k W_{jk} \\delta^o_k f'(h_j)$. But since we have one hidden unit and one output unit, this is much simpler.\n",
    "$$\\delta^h = W \\delta^o f'(h) = 0.1 \\times 0.122 \\times 0.495 \\times (1 - 0.495) = 0.003$$\n",
    "\n",
    "Now that we have the errors, we can calculate the gradient descent steps. The hidden to output weight step is the learning rate, times the output unit error, times the hidden unit activation value.\n",
    "\n",
    "$$\\Delta W = \\eta  \\delta^o a = 0.5 \\times 0.122 \\times 0.495 = 0.0302$$\n",
    "\n",
    "Then, for the input to hidden weights $w_i$, it's the learning rate times the hidden unit error, times the input values.\n",
    "$$\\Delta w_i = \\eta \\delta^h x_i = (0.5 \\times 0.003 \\times 0.1, 0.5 \\times 0.003 \\times 0.3) = (0.00015, 0.00045)$$\n",
    "\n",
    "From this example, you can see one of the effects of using the sigmoid function for the activations. The maximum derivative of the sigmoid function is 0.25, so the errors in the output layer get reduced by at least 75%, and errors in the hidden layer are scaled down by at least 93.75%! You can see that if you have a lot of layers, using a sigmoid activation function will quickly reduce the weight steps to tiny values in layers near the input. This is known as the _vanishing gradient problem_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a Neural Network with a Hidden Layer\n",
    "Now you're going to implement the backprop algorithm for a network trained on the graduate school admission data. You should have everything you need from the previous exercises to complete this one.\n",
    "\n",
    "$\\delta_k = (y_k - \\hat y_k) f'(a_k)$\n",
    "\n",
    "For now we'll only consider a simple network with one hidden layer and one output unit. \n",
    "Here's the general algorithm for updating the weights with backpropagation:\n",
    "\n",
    "- Set the weight steps for each layer to zero\n",
    "    - The input to hidden weights $Δw_{ij} = 0$\n",
    "    - The hidden to output weights $ΔW_j = 0$\n",
    "- For each record in the training data:\n",
    "    - Make a forward pass through the network, calculating the output $y$ \n",
    "    - Calculate the error gradient in the output unit: $\\delta^o = (y - \\hat y)  f'(z)$, where $z = \\sum_j W_j a_j$, the input to the output unit.\n",
    "    - Propagate the errors to the hidden layer $\\delta^h_j = \\delta^o W_j  f'(h_j)$\n",
    "    - Update the weight steps:\n",
    "        - $\\Delta W_j = \\Delta W_j + \\delta^o a_j$\n",
    "        - $\\Delta w_{ij} = \\Delta w_{ij} + \\delta^h_j a_i$ \n",
    "- Update the weights, where $\\eta$ is the learning rate and $m$ is the number of records:\n",
    "    - $W_j = W_j  + \\eta \\Delta W_j / m$\n",
    "    - $w_{ij} = w_{ij}  + \\eta \\Delta w_{ij} / m$\n",
    "- Repeat for $e$ epochs.\n",
    "\n",
    "Your goals here:\n",
    "- Implement the forward pass.\n",
    "- Implement the backpropagation algorithm.\n",
    "- Update the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change in weights for hidden layer to output layer:\n",
      "[ 0.00804047  0.00555918]\n",
      "Change in weights for input layer to hidden layer:\n",
      "[[  1.77005547e-04  -5.11178506e-04]\n",
      " [  3.54011093e-05  -1.02235701e-04]\n",
      " [ -7.08022187e-05   2.04471402e-04]]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "x = np.array([0.5, 0.1, -0.2])\n",
    "target = 0.6\n",
    "learnrate = 0.5\n",
    "\n",
    "weights_input_hidden = np.array([[0.5, -0.6],\n",
    "                                 [0.1, -0.2],\n",
    "                                 [0.1, 0.7]])\n",
    "\n",
    "weights_hidden_output = np.array([0.1, -0.3])\n",
    "\n",
    "## Forward pass\n",
    "hidden_layer_input = np.dot(x, weights_input_hidden)\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "output = sigmoid(output_layer_in)\n",
    "\n",
    "## Backwards pass\n",
    "## Calculate error\n",
    "error = target - output\n",
    "\n",
    "# Calculate error gradient for output layer\n",
    "del_err_output = error * output * (1 - output)\n",
    "\n",
    "# Calculate error gradient for hidden layer\n",
    "del_err_hidden = np.dot(del_err_output, weights_hidden_output) * \\\n",
    "                 hidden_layer_output * (1 - hidden_layer_output)\n",
    "\n",
    "# Calculate change in weights for hidden layer to output layer\n",
    "delta_w_h_o = learnrate * del_err_output * hidden_layer_output\n",
    "\n",
    "# Calculate change in weights for input layer to hidden layer\n",
    "delta_w_i_o = learnrate * del_err_hidden * x[:, None]\n",
    "\n",
    "print('Change in weights for hidden layer to output layer:')\n",
    "print(delta_w_h_o)\n",
    "print('Change in weights for input layer to hidden layer:')\n",
    "print(delta_w_i_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}